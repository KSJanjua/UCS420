{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e66b538-8eef-49e1-8b7d-895f3ecf03dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\janju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\janju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\janju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\janju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d195465-19ed-499a-8a6e-631f0c9f9c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQuestion 1:\\nWrite a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.). \\n    1. Convert text to lowercase and remove punctuation. \\n    2. Tokenize the text into words and sentences. \\n    3. Remove stopwords (using NLTK's stopwords list). \\n    4. Display word frequency distribu on (excluding stopwords). \\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Question 1:\n",
    "Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.). \n",
    "    1. Convert text to lowercase and remove punctuation. \n",
    "    2. Tokenize the text into words and sentences. \n",
    "    3. Remove stopwords (using NLTK's stopwords list). \n",
    "    4. Display word frequency distribu on (excluding stopwords). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a17d50bd-75c0-435d-978d-537b90e8fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Technology continues to reshape our lives in fascinating ways. \n",
      "        From artificial intelligence to quantum computing, innovations are evolving faster than ever. \n",
      "        These advancements improve healthcare, boost productivity, and even change how we communicate. \n",
      "        However, with progress also come challenges like data privacy and ethical concerns.\n",
      "        Staying informed and adaptable is essential in this rapidly shifting landscape.\n",
      "        Hello, I-am-karan\n",
      "        it isn't the case\n",
      "        +91 9337163948\n",
      "        karan@gmail.com\n",
      "        http://karan.com\n",
      "        \n",
      "\n",
      "Text in lowercase:\n",
      "technology continues to reshape our lives in fascinating ways. \n",
      "        from artificial intelligence to quantum computing, innovations are evolving faster than ever. \n",
      "        these advancements improve healthcare, boost productivity, and even change how we communicate. \n",
      "        however, with progress also come challenges like data privacy and ethical concerns.\n",
      "        staying informed and adaptable is essential in this rapidly shifting landscape.\n",
      "        hello, i-am-karan\n",
      "        it isn't the case\n",
      "        +91 9337163948\n",
      "        karan@gmail.com\n",
      "        http://karan.com\n",
      "        \n",
      "\n",
      "Text without punctutations:\n",
      "technology continues to reshape our lives in fascinating ways \n",
      "        from artificial intelligence to quantum computing innovations are evolving faster than ever \n",
      "        these advancements improve healthcare boost productivity and even change how we communicate \n",
      "        however with progress also come challenges like data privacy and ethical concerns\n",
      "        staying informed and adaptable is essential in this rapidly shifting landscape\n",
      "        hello iamkaran\n",
      "        it isnt the case\n",
      "        91 9337163948\n",
      "        karangmailcom\n",
      "        httpkarancom\n",
      "        \n",
      "Tokenize into sentences:\n",
      "['technology continues to reshape our lives in fascinating ways \\n        from artificial intelligence to quantum computing innovations are evolving faster than ever \\n        these advancements improve healthcare boost productivity and even change how we communicate \\n        however with progress also come challenges like data privacy and ethical concerns\\n        staying informed and adaptable is essential in this rapidly shifting landscape\\n        hello iamkaran\\n        it isnt the case\\n        91 9337163948\\n        karangmailcom\\n        httpkarancom']\n",
      "Tokenize into word:\n",
      "['technology', 'continues', 'to', 'reshape', 'our', 'lives', 'in', 'fascinating', 'ways', 'from', 'artificial', 'intelligence', 'to', 'quantum', 'computing', 'innovations', 'are', 'evolving', 'faster', 'than', 'ever', 'these', 'advancements', 'improve', 'healthcare', 'boost', 'productivity', 'and', 'even', 'change', 'how', 'we', 'communicate', 'however', 'with', 'progress', 'also', 'come', 'challenges', 'like', 'data', 'privacy', 'and', 'ethical', 'concerns', 'staying', 'informed', 'and', 'adaptable', 'is', 'essential', 'in', 'this', 'rapidly', 'shifting', 'landscape', 'hello', 'iamkaran', 'it', 'isnt', 'the', 'case', '91', '9337163948', 'karangmailcom', 'httpkarancom']\n",
      "Tokenized Sentences:\n",
      "['technology continues to reshape our lives in fascinating ways \\n        from artificial intelligence to quantum computing innovations are evolving faster than ever \\n        these advancements improve healthcare boost productivity and even change how we communicate \\n        however with progress also come challenges like data privacy and ethical concerns\\n        staying informed and adaptable is essential in this rapidly shifting landscape\\n        hello iamkaran\\n        it isnt the case\\n        91 9337163948\\n        karangmailcom\\n        httpkarancom']\n",
      "\n",
      "Filtered Words:\n",
      "['technology', 'continues', 'reshape', 'lives', 'fascinating', 'ways', 'artificial', 'intelligence', 'quantum', 'computing', 'innovations', 'evolving', 'faster', 'ever', 'advancements', 'improve', 'healthcare', 'boost', 'productivity', 'even', 'change', 'communicate', 'however', 'progress', 'also', 'come', 'challenges', 'like', 'data', 'privacy', 'ethical', 'concerns', 'staying', 'informed', 'adaptable', 'essential', 'rapidly', 'shifting', 'landscape', 'hello', 'iamkaran', 'isnt', 'case', '91', '9337163948', 'karangmailcom', 'httpkarancom']\n",
      "\n",
      "Word Frequency Distribution:\n",
      "technology: 1\n",
      "continues: 1\n",
      "reshape: 1\n",
      "lives: 1\n",
      "fascinating: 1\n",
      "ways: 1\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "quantum: 1\n",
      "computing: 1\n",
      "innovations: 1\n",
      "evolving: 1\n",
      "faster: 1\n",
      "ever: 1\n",
      "advancements: 1\n",
      "improve: 1\n",
      "healthcare: 1\n",
      "boost: 1\n",
      "productivity: 1\n",
      "even: 1\n",
      "change: 1\n",
      "communicate: 1\n",
      "however: 1\n",
      "progress: 1\n",
      "also: 1\n",
      "come: 1\n",
      "challenges: 1\n",
      "like: 1\n",
      "data: 1\n",
      "privacy: 1\n",
      "ethical: 1\n",
      "concerns: 1\n",
      "staying: 1\n",
      "informed: 1\n",
      "adaptable: 1\n",
      "essential: 1\n",
      "rapidly: 1\n",
      "shifting: 1\n",
      "landscape: 1\n",
      "hello: 1\n",
      "iamkaran: 1\n",
      "isnt: 1\n",
      "case: 1\n",
      "91: 1\n",
      "9337163948: 1\n",
      "karangmailcom: 1\n",
      "httpkarancom: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Technology continues to reshape our lives in fascinating ways. \n",
    "        From artificial intelligence to quantum computing, innovations are evolving faster than ever. \n",
    "        These advancements improve healthcare, boost productivity, and even change how we communicate. \n",
    "        However, with progress also come challenges like data privacy and ethical concerns.\n",
    "        Staying informed and adaptable is essential in this rapidly shifting landscape.\n",
    "        Hello, I-am-karan\n",
    "        it isn't the case\n",
    "        +91 9337163948\n",
    "        karan@gmail.com\n",
    "        http://karan.com\n",
    "        \"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "#convert text to lowercase\n",
    "text_lower = text.lower()\n",
    "print(\"\\nText in lowercase:\")\n",
    "print(text_lower)\n",
    "\n",
    "#remove punctutations\n",
    "text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "print(\"\\nText without punctutations:\")\n",
    "print(text_no_punct)\n",
    "\n",
    "#tokenize into sentences\n",
    "sentences = sent_tokenize(text_no_punct)\n",
    "print(\"Tokenize into sentences:\")\n",
    "print(sentences)\n",
    "\n",
    "#tokenize into words\n",
    "words = word_tokenize(text_no_punct)\n",
    "print(\"Tokenize into word:\")\n",
    "print(words)\n",
    "\n",
    "#remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "#frequency distribution\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(sentences)\n",
    "print(\"\\nFiltered Words:\")\n",
    "print(filtered_words)\n",
    "print(\"\\nWord Frequency Distribution:\")\n",
    "for word, freq in freq_dist.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c2edcf0-2894-4c3c-bbb7-4af83ee293c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer:\n",
      " ['technolog', 'continu', 'reshap', 'live', 'fascin', 'way', 'artifici', 'intellig', 'quantum', 'comput', 'innov', 'evolv', 'faster', 'ever', 'advanc', 'improv', 'healthcar', 'boost', 'product', 'even', 'chang', 'commun', 'howev', 'progress', 'also', 'come', 'challeng', 'like', 'data', 'privaci', 'ethic', 'concern', 'stay', 'inform', 'adapt', 'essenti', 'rapidli', 'shift', 'landscap', 'hello', 'iamkaran', 'isnt', 'case', '91', '9337163948', 'karangmailcom', 'httpkarancom']\n",
      "\n",
      "Lancaster Stemmer:\n",
      " ['technolog', 'continu', 'reshap', 'liv', 'fascin', 'way', 'art', 'intellig', 'quant', 'comput', 'innov', 'evolv', 'fast', 'ev', 'adv', 'improv', 'healthc', 'boost', 'produc', 'ev', 'chang', 'commun', 'howev', 'progress', 'also', 'com', 'challeng', 'lik', 'dat', 'priv', 'eth', 'concern', 'stay', 'inform', 'adapt', 'ess', 'rapid', 'shift', 'landscap', 'hello', 'iamk', 'isnt', 'cas', '91', '9337163948', 'karangmailcom', 'httpkarancom']\n",
      "\n",
      "Lemmatization:\n",
      " ['technology', 'continues', 'reshape', 'life', 'fascinating', 'way', 'artificial', 'intelligence', 'quantum', 'computing', 'innovation', 'evolving', 'faster', 'ever', 'advancement', 'improve', 'healthcare', 'boost', 'productivity', 'even', 'change', 'communicate', 'however', 'progress', 'also', 'come', 'challenge', 'like', 'data', 'privacy', 'ethical', 'concern', 'staying', 'informed', 'adaptable', 'essential', 'rapidly', 'shifting', 'landscape', 'hello', 'iamkaran', 'isnt', 'case', '91', '9337163948', 'karangmailcom', 'httpkarancom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2: Stemming and Lemmatization on \n",
    "    1. Take the tokenized words from Question 1 (after stopword removal). \n",
    "    2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer. \n",
    "    3. Apply lemma za on using NLTK's WordNetLemma zer. \n",
    "    4. Compare and display results of both techniques. \n",
    "\"\"\"\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "porter_stemmed = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\nPorter Stemmer:\\n\", porter_stemmed)\n",
    "print(\"\\nLancaster Stemmer:\\n\", lancaster_stemmed)\n",
    "print(\"\\nLemmatization:\\n\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cfaca5d-bda9-4a4a-9099-60584edcaacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with more than 5 letters:\n",
      " ['Technology', 'continues', 'reshape', 'fascinating', 'artificial', 'intelligence', 'quantum', 'computing', 'innovations', 'evolving', 'faster', 'advancements', 'improve', 'healthcare', 'productivity', 'change', 'communicate', 'However', 'progress', 'challenges', 'privacy', 'ethical', 'concerns', 'Staying', 'informed', 'adaptable', 'essential', 'rapidly', 'shifting', 'landscape', '9337163948']\n",
      "\n",
      "Numbers extracted:\n",
      " ['91', '9337163948']\n",
      "\n",
      "Capitalized words:\n",
      " ['Technology', 'From', 'These', 'However', 'Staying', 'Hello', 'I']\n",
      "\n",
      "Alphabetic words only:\n",
      " ['Technology', 'continues', 'to', 'reshape', 'our', 'lives', 'in', 'fascinating', 'ways', 'From', 'artificial', 'intelligence', 'to', 'quantum', 'computing', 'innovations', 'are', 'evolving', 'faster', 'than', 'ever', 'These', 'advancements', 'improve', 'healthcare', 'boost', 'productivity', 'and', 'even', 'change', 'how', 'we', 'communicate', 'However', 'with', 'progress', 'also', 'come', 'challenges', 'like', 'data', 'privacy', 'and', 'ethical', 'concerns', 'Staying', 'informed', 'and', 'adaptable', 'is', 'essential', 'in', 'this', 'rapidly', 'shifting', 'landscape', 'Hello', 'I', 'am', 'karan', 'it', 'isn', 't', 'the', 'case', 'karan', 'gmail', 'com', 'http', 'karan', 'com']\n",
      "\n",
      "Words starting with vowels:\n",
      " ['our', 'in', 'artificial', 'intelligence', 'innovations', 'are', 'evolving', 'ever', 'advancements', 'improve', 'and', 'even', 'also', 'and', 'ethical', 'informed', 'and', 'adaptable', 'is', 'essential', 'in', 'I', 'am', 'it', 'isn']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q3. Regular Expressions and Text Spli ng \n",
    "    1. Take their original text from Ques on 1. \n",
    "    2. Use regular expressions to: \n",
    "        a. Extract all words with more than 5 le ers.\n",
    "        b. Extract all numbers (if any exist in their text). \n",
    "        c. Extract all capitalized words. \n",
    "    3. Use text spli ng techniques to: \n",
    "        a. Split the text into words containing only alphabets (removing digits and special characters). \n",
    "        b. Extract words star ng with a vowel. \n",
    "\"\"\"\n",
    "\n",
    "# a. Extract all words with more than 5 letters\n",
    "words_5plus = re.findall(r'\\b\\w{6,}\\b', text)\n",
    "print(\"\\nWords with more than 5 letters:\\n\", words_5plus)\n",
    "\n",
    "# b. Extract all numbers\n",
    "numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "print(\"\\nNumbers extracted:\\n\", numbers)\n",
    "\n",
    "# c. Extract all capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "print(\"\\nCapitalized words:\\n\", capitalized_words)\n",
    "\n",
    "# Text Splitting\n",
    "\n",
    "# a. Split into only alphabetic words\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "print(\"\\nAlphabetic words only:\\n\", alpha_words)\n",
    "\n",
    "# b. Extract words starting with a vowel\n",
    "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
    "print(\"\\nWords starting with vowels:\\n\", vowel_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a2d037d-b9cc-429a-866f-9758b7eb727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Tokenized Words:\n",
      " ['Technology', 'continues', 'to', 'reshape', 'our', 'lives', 'in', 'fascinating', 'ways', 'From', 'artificial', 'intelligence', 'to', 'quantum', 'computing', 'innovations', 'are', 'evolving', 'faster', 'than', 'ever', 'These', 'advancements', 'improve', 'healthcare', 'boost', 'productivity', 'and', 'even', 'change', 'how', 'we', 'communicate', 'However', 'with', 'progress', 'also', 'come', 'challenges', 'like', 'data', 'privacy', 'and', 'ethical', 'concerns', 'Staying', 'informed', 'and', 'adaptable', 'is', 'essential', 'in', 'this', 'rapidly', 'shifting', 'landscape', 'Hello', 'I-am-karan', 'it', 'isn', 't', 'the', 'case', '91', '9337163948', 'karan', 'gmail', 'com', 'http', 'karan', 'com']\n",
      "\n",
      "Text after Replacements:\n",
      " Technology continues to reshape our lives in fascinating ways. \n",
      "        From artificial intelligence to quantum computing, innovations are evolving faster than ever. \n",
      "        These advancements improve healthcare, boost productivity, and even change how we communicate. \n",
      "        However, with progress also come challenges like data privacy and ethical concerns.\n",
      "        Staying informed and adaptable is essential in this rapidly shifting landscape.\n",
      "        Hello, I-am-karan\n",
      "        it isn't the case\n",
      "        <PHONE>\n",
      "        <EMAIL>\n",
      "        <URL>\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4. Custom Tokeniza on & Regex-based Text Cleaning \n",
    "    1. Take original text from Ques on 1. \n",
    "    2. Write a custom tokeniza on func on that: \n",
    "        a. Removes punctua on and special symbols, but keeps contrac ons (e.g., \"isn't\" should not be split into \"is\" and \"n't\"). \n",
    "        b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token). \n",
    "        c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is). \n",
    "    3. Use Regex Subs tu ons (re.sub) to: \n",
    "        a. Replace email addresses with '<EMAIL>' placeholder. \n",
    "        b. Replace URLs with '<URL>' placeholder. \n",
    "        c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '<PHONE>' placeholder.\n",
    "\"\"\"\n",
    "\n",
    "# Custom Tokenizer\n",
    "def custom_tokenizer(text):\n",
    "    # Keep contractions and hyphenated words, keep decimals intact\n",
    "    pattern = r\"\\b(?:\\d+\\.\\d+|\\w+(?:-\\w+)*|\\w+'\\w+)\\b\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens\n",
    "\n",
    "tokens_custom = custom_tokenizer(text)\n",
    "print(\"\\nCustom Tokenized Words:\\n\", tokens_custom)\n",
    "\n",
    "# Regex Substitutions\n",
    "\n",
    "# a. Replace Email Addresses\n",
    "text_email_replaced = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)\n",
    "\n",
    "# b. Replace URLs\n",
    "text_url_replaced = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text_email_replaced)\n",
    "\n",
    "# c. Replace Phone Numbers\n",
    "text_phone_replaced = re.sub(r'(\\+?\\d{1,3}[-.\\s]?\\d{3}[-.\\s]?\\d{4,7})', '<PHONE>', text_url_replaced)\n",
    "\n",
    "print(\"\\nText after Replacements:\\n\", text_phone_replaced)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
